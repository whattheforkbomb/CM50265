{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {},
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ac481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_cache = {'Train_path':'', 'Test_path':'', 'X_train':None, 'y_train':None, 'X_test': None, 'y_train':None}\n",
    "\n",
    "def clear_cache():\n",
    "    data_cache['Test_path'] = ''\n",
    "    data_cache['Train_path'] = ''\n",
    "    data_cache['X_train'] = None\n",
    "    data_cache['y_train'] = None\n",
    "    data_cache['X_test'] = None\n",
    "    data_cache['y_test'] = None\n",
    "    \n",
    "def load_data(train_file, test_file):\n",
    "    # Read the CSV file and extract Bag of Words Features\n",
    "    \n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    raw_X_train = list(train_df.review)\n",
    "    y_train = list(map( (lambda x : 0 if x == 'negative' else 1), train_df.sentiment))\n",
    "    \n",
    "    raw_X_test = list(test_df.review)\n",
    "    y_test = list(map( (lambda x : 0 if x == 'negative' else 1), test_df.sentiment))\n",
    "    \n",
    "    return raw_train_x, train_y, raw_test_x, test_y \n",
    "\n",
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    if(data_cache['X_train'] is not None \n",
    "        and data_cache['y_train'] is not None\n",
    "        and data_cache['X_test']is not None\n",
    "        and data_cache['y_test'] is not None\n",
    "        and data_cache['Train_path'] == train_file\n",
    "        and data_cache['Test_path'] == test_file):\n",
    "        print('Getting from cache')\n",
    "        return (data_cache['X_train'], data_cache['y_train'], data_cache['X_test'], data_cache['y_test'])\n",
    "\n",
    "    # Read the CSV file and extract Bag of Words Features\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    raw_X_train = list(train_df.review)\n",
    "    y_train = list(map( (lambda x : 0 if x == 'negative' else 1), train_df.sentiment))\n",
    "    \n",
    "    raw_X_test = list(test_df.review)\n",
    "    y_test = list(map( (lambda x : 0 if x == 'negative' else 1), test_df.sentiment))\n",
    "    \n",
    "    X_train = get_embeddings(raw_X_train)\n",
    "    X_test  = get_embeddings(raw_X_test)\n",
    "    \n",
    "    data_cache['Test_path'] = test_file\n",
    "    data_cache['Train_path'] = train_file\n",
    "    data_cache['X_train'] = X_train\n",
    "    data_cache['y_train'] = y_train\n",
    "    data_cache['X_test'] = X_test\n",
    "    data_cache['y_test'] = y_test\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "# def install_embedding_model():\n",
    "#     import importlib.util\n",
    "#     import subprocess\n",
    "#     import sys\n",
    "   \n",
    "#     package_name = 'sentence-transformers'\n",
    "\n",
    "#     #check if already installed\n",
    "#     spec = importlib.util.find_spec(package_name)\n",
    "#     if spec is None:\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "\n",
    "def get_embeddings(reviews):\n",
    "    #\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')#'sentence-transformers/paraphrase-MiniLM-L3-v2')\n",
    "    embeddings = model.encode(reviews)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "train_file = 'movie_review_test.csv'\n",
    "test_file =  'movie_review_test.csv'\n",
    "\n",
    "#x_test, y_train, x_test, y_test = extract_bag_of_words_train_test(train_file, test_file)\n",
    "#embeddings = get_embeddings(x)\n",
    "\n",
    "# print(len(x), len(y))\n",
    "# print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "install_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68b1dc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(769,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94c07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #implement initialisation\n",
    "        #self.some_paramter=1\n",
    "        self.clf = svm.SVC(decision_function_shape='ovo')\n",
    "        self.clf.kernel = 'rbf'\n",
    "        \n",
    "    # define your own kernel here\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html\n",
    "    def fit(self, X,y):\n",
    "        # training of the SVM\n",
    "        # Ensure you call your own defined kernel here\n",
    "\n",
    "        self.clf.fit(X, y)\n",
    "               \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        predictions = self.clf.predict(X)\n",
    "        #print('predictions:', len(predictions))\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89603f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test_func_svm(dataset_train, dataset_test):    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    #print(X_train, Y_train)\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    #print(\"Accuracy:\",acc)\n",
    "    return acc\n",
    "\n",
    "def test_svm(model, train_x, train_y, test_x, test_y):  \n",
    "    sc = model \n",
    "    #print(X_train, Y_train)\n",
    "    sc.fit(train_x, train_y)\n",
    "    Y_Pred = sc.predict(test_x)\n",
    "    acc = accuracy_score(test_y, Y_Pred)\n",
    "    #print(\"Accuracy:\",acc)\n",
    "    return acc\n",
    "\n",
    "def crossval_svm(model, x, y, kfolds = 5):\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    kfold_accuracys = []\n",
    "    \n",
    "    kf = KFold(n_splits = kfolds)\n",
    "    train_x = kf.split(x,y)\n",
    "    #print(train_x)\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in kf.split(x):\n",
    "        train_x = x[train_index]\n",
    "        train_y = y[train_index]\n",
    "        val_x = x[val_index]\n",
    "        val_y = y[val_index]\n",
    "        \n",
    "        acc = test_svm(model, train_x, train_y, val_x, val_y)\n",
    "        #print(acc)\n",
    "        kfold_accuracys.append(acc)\n",
    "        \n",
    "    return np.mean(kfold_accuracys)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ac1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "#X_train, y_train, X_test, y_test  = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ffd4adf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Hyperparams chosen with grid search, need to implement params in the wrapper constructor\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model.clf = svm.SVC(decision_function_shape='ovo', kernel = 'rbf', gamma = 1, C = 1)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mclf \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(decision_function_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m, degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, C \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m cross_val_acc \u001b[38;5;241m=\u001b[39m crossval_svm(model, \u001b[43mX_train\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(y_train))\n\u001b[1;32m      9\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m test_func_svm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie_review_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmovie_review_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross val accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, cross_val_acc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, test_acc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#\n",
    "model = SVMClassifier()\n",
    "#Hyperparams chosen with grid search, need to implement params in the wrapper constructor\n",
    "#model.clf = svm.SVC(decision_function_shape='ovo', kernel = 'rbf', gamma = 1, C = 1)\n",
    "\n",
    "model.clf = svm.SVC(decision_function_shape='ovo', kernel = 'poly', degree = 1, C = 1)\n",
    "cross_val_acc = crossval_svm(model, X_train, np.array(y_train))\n",
    "\n",
    "test_acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "\n",
    "print(\"Cross val accuracy: \", cross_val_acc, \"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12d08be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5488.145039273248, 'gamma': 7151.903663724195}\n"
     ]
    }
   ],
   "source": [
    "### rbf hyperparam tuning - very slow do not run - it resulted in c=1, gamma=1\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "\n",
    "C_range = 10. ** np.linspace(-3, 3, 15)\n",
    "gamma_range = 10. ** np.linspace(-3,4, 15)\n",
    "distributions = dict(C = uniform(loc=10e-3, scale=10e3), gamma = uniform(loc=10e-3, scale=10e3))\n",
    "\n",
    "#hyperparameters = {'C':C_range, 'gamma':gamma_range}\n",
    "svc = SVMClassifier()\n",
    "tuner = RandomizedSearchCV(svm.SVC(), distributions, scoring='accuracy', n_iter=20, random_state=0)\n",
    "tuner.fit(X_train, y_train)\n",
    "\n",
    "print(tuner.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb2d81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'degree': 1.0, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "### polynomial hyperparam tuning - it chose 1, 1 again\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "\n",
    "C_range = 10. ** np.linspace(-3, 3, 15)\n",
    "degree_range = np.linspace(1, 20, 6)\n",
    "\n",
    "hyperparameters = {'kernel': ['poly'], 'C':C_range, 'degree':degree_range}\n",
    "svc = SVMClassifier()\n",
    "tuner = GridSearchCV(svc.clf, hyperparameters,scoring = 'accuracy', cv=StratifiedKFold(5))\n",
    "tuner.fit(X_train, y_train)\n",
    "\n",
    "print(tuner.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3805e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "import math\n",
    "import random\n",
    "\n",
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    def __init__(self, num_classifiers = 50, max_depth = 3):\n",
    "        import numpy as np\n",
    "        #implement initialisation\n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.max_depth = max_depth\n",
    "        self.classifiers = []\n",
    "        self.alphas = np.ones(self.num_classifiers)\n",
    "        self.alphas /= self.num_classifiers\n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        signed_y = [1 if x == 1 else -1 for x in y]\n",
    "        num_samples = X.shape[0]\n",
    "        w = np.random.rand(num_samples)\n",
    "        w /= num_samples\n",
    "        alph = np.ones(self.num_classifiers)\n",
    "        #print(signed_y)\n",
    "        for c in range(self.num_classifiers):\n",
    "#             plt.hist(w)\n",
    "#             plt.show()\n",
    "            \n",
    "            clf = tree.DecisionTreeClassifier(max_depth = self.max_depth)#splitter='random')\n",
    "            self.classifiers.append(clf)\n",
    "            \n",
    "#             random_w = np.random.rand(num_samples)\n",
    "#             random_w /= np.sum(random_w)\n",
    "            clf.fit(X, signed_y, sample_weight = w)\n",
    "            \n",
    "            #get predictions and calculate error rate\n",
    "            predictions = clf.predict(X)     \n",
    "            error_rate = np.count_nonzero(predictions != signed_y) / num_samples\n",
    "            #print(error_rate)\n",
    "            #print('training:', h[:5])\n",
    "            #set classifier weight\n",
    "            alpha = 0.5 * math.log((1-error_rate)/error_rate) if error_rate > 0 else 1\n",
    "            self.alphas[c]  = alpha\n",
    "            #update sample weights\n",
    "            \n",
    "            new_w = np.copy(w)\n",
    "            for i in range(num_samples):\n",
    "                w[i] = w[i] * math.exp(-1 * alpha * signed_y[i] * predictions[i])\n",
    "                \n",
    "            w /= np.sum(new_w)\n",
    "        \n",
    "#         plt.hist(w)\n",
    "#         plt.show() \n",
    "#         plt.hist(self.alphas)\n",
    "#         plt.show()\n",
    "        #implement training of the boosting classifier\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep= False):\n",
    "        return {'num_classifiers':self.num_classifiers, 'max_depth':self.max_depth}\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # implement prediction of the boosting classifier\n",
    "        h = []\n",
    "        accum = []\n",
    "        for x in X:\n",
    "            cumsum = 0\n",
    "            for c in range(self.num_classifiers):\n",
    "                cumsum += self.alphas[c] * self.classifiers[c].predict(np.array(x).reshape(1,-1))\n",
    "                accum.append(cumsum)\n",
    "                #print(cumsum)\n",
    "            h.append(np.sign(cumsum))\n",
    "        #print('predicting:', h[:5])\n",
    "        return [0 if x < 0 else 1 for x in h]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4632591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    X_train, Y_train, X_test, Y_test = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    #print(X_test.shape)\n",
    "    Y_Pred = bc.predict(X_test)\n",
    "    #print(np.array(Y_test).shape, np.array(Y_Pred).shape)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c27de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_func_boosting\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovie_review_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovie_review_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc:\u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mtest_func_boosting\u001b[0;34m(dataset_train, dataset_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_func_boosting\u001b[39m(dataset_train, dataset_test):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score    \n\u001b[0;32m----> 3\u001b[0m     X_train, Y_train, X_test, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mextract_bag_of_words_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     bc \u001b[38;5;241m=\u001b[39m BoostingClassifier()\n\u001b[1;32m      5\u001b[0m     bc\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mextract_bag_of_words_train_test\u001b[0;34m(train_file, test_file)\u001b[0m\n\u001b[1;32m     47\u001b[0m raw_X_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(test_df\u001b[38;5;241m.\u001b[39mreview)\n\u001b[1;32m     48\u001b[0m y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m( (\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m), test_df\u001b[38;5;241m.\u001b[39msentiment))\n\u001b[0;32m---> 50\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_X_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m X_test  \u001b[38;5;241m=\u001b[39m get_embeddings(raw_X_test)\n\u001b[1;32m     53\u001b[0m data_cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_file\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     77\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L12-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#'sentence-transformers/paraphrase-MiniLM-L3-v2')\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:187\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 187\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    191\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca42a29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score  \n",
    "def crossval_boost(model, x, y, kfolds = 5):\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    kfold_accuracys = []\n",
    "    \n",
    "    kf = KFold(n_splits = kfolds)\n",
    "    train_x = kf.split(x,y)\n",
    "    #print(train_x)\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in kf.split(x):\n",
    "        #print(train_index, val_index)\n",
    "        train_x = x[train_index]\n",
    "        train_y = y[train_index]\n",
    "        val_x = x[val_index]\n",
    "        val_y = y[val_index]\n",
    "        \n",
    "        bc = BoostingClassifier()\n",
    "        bc.fit(train_x, train_y)\n",
    "        Y_Pred = bc.predict(val_x)\n",
    "        acc = accuracy_score(val_y, Y_Pred)\n",
    "        \n",
    "        #print(acc)\n",
    "        kfold_accuracys.append(acc)\n",
    "        \n",
    "    return np.mean(kfold_accuracys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e714ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparam tuning for gradboost\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "#X_train, y_train, X_test, Y_test = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "\n",
    "\n",
    "def grid_search(params):\n",
    "    X_train, y_train, X_test, Y_test = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "    clfs = []\n",
    "    best_acc = 0\n",
    "\n",
    "    for num in params['num_classifiers']:\n",
    "        for max_depth in params['max_depth']:\n",
    "            clfs.append(BoostingClassifier(num, max_depth))\n",
    "    for clf in clfs:\n",
    "        accuracy = crossval_boost(clf, X_train, np.array(y_train), 5)\n",
    "        print('Acc:' , accuracy, 'Params:', clf.num_classifiers, clf.max_depth)\n",
    "        if accuracy > best_acc:\n",
    "            best_clf = clf\n",
    "            best_acc = accuracy\n",
    "            \n",
    "    print(\"Best Params:\", best_clf.num_classifiers, best_clf.max_depth)\n",
    "    return best_clf\n",
    "    \n",
    "grid = {'num_classifiers': [50, 100, 500], 'max_depth': [2, 5, 10]}\n",
    "best_clf = grid_search(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58afdc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review sentiment  Unnamed: 2\n",
      "0     Not a movie for everyone, but this movie is in...  positive         NaN\n",
      "1     This film is not your typical Hollywood fare, ...  positive         NaN\n",
      "2     RKO Radio Pictures made a real classic in 1947...  positive         NaN\n",
      "3     \"Transylvania 6-5000\" is an insignificant but ...  negative         NaN\n",
      "4     This is a very good, under-rated action/drama/...  positive         NaN\n",
      "...                                                 ...       ...         ...\n",
      "1495  The essential message - one which Miller would...  positive         NaN\n",
      "1496  This movie is tremendous for uplifting the Spi...  positive         NaN\n",
      "1497  I think this movie is different apart from mos...  positive         NaN\n",
      "1498  Brit director Chrstopher Nolan now has a caree...  positive         NaN\n",
      "1499  The Salena Incident is set in Arizona where si...  negative         NaN\n",
      "\n",
      "[1500 rows x 3 columns]\n",
      "                                                 review sentiment  Unnamed: 2\n",
      "1     This film is not your typical Hollywood fare, ...  positive         NaN\n",
      "2     RKO Radio Pictures made a real classic in 1947...  positive         NaN\n",
      "3     \"Transylvania 6-5000\" is an insignificant but ...  negative         NaN\n",
      "5     I almost made a fool of myself when I was goin...  negative         NaN\n",
      "6     \"You got any beans? Beans is good. You just ea...  negative         NaN\n",
      "...                                                 ...       ...         ...\n",
      "1494  I am new at this, so bear with me please. I am...  positive         NaN\n",
      "1495  The essential message - one which Miller would...  positive         NaN\n",
      "1496  This movie is tremendous for uplifting the Spi...  positive         NaN\n",
      "1497  I think this movie is different apart from mos...  positive         NaN\n",
      "1498  Brit director Chrstopher Nolan now has a caree...  positive         NaN\n",
      "\n",
      "[1072 rows x 3 columns]\n",
      "                                                 review sentiment  Unnamed: 2\n",
      "0     Not a movie for everyone, but this movie is in...  positive         NaN\n",
      "4     This is a very good, under-rated action/drama/...  positive         NaN\n",
      "8     When this movie was first shown on television ...  negative         NaN\n",
      "14    Flashes of lightning; a sprawling cemetery; th...  positive         NaN\n",
      "16    If you're familiar with the work of auteur Joh...  positive         NaN\n",
      "...                                                 ...       ...         ...\n",
      "1486  This movie was just as good as some of the oth...  positive         NaN\n",
      "1488  Ah, Batman Returns, is it possible to have a s...  positive         NaN\n",
      "1490  \"Four Daughters,\" a sentimental story of a sol...  positive         NaN\n",
      "1492  I just watched this movie on Showtime. Quite b...  positive         NaN\n",
      "1499  The Salena Incident is set in Arizona where si...  negative         NaN\n",
      "\n",
      "[428 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'misclass_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(correct_samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 35\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mmisclass_samples\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'misclass_samples' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, Y_test = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "raw_X_test = pd.read_csv(\"movie_review_test.csv\")\n",
    "\n",
    "\n",
    "bc = BoostingClassifier()\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "Y_Pred = bc.predict(X_test)\n",
    "print(raw_X_test)\n",
    "\n",
    "correct_indices = np.nonzero(np.equal(Y_Pred, Y_test))\n",
    "misclassified_indices = np.nonzero(np.not_equal(Y_Pred, Y_test))\n",
    "\n",
    "correct_samples = raw_X_test.iloc[correct_indices]\n",
    "misclassified_samples = raw_X_test.iloc[misclassified_indices]\n",
    "\n",
    "print(correct_samples)\n",
    "print(misclassified_samples)\n",
    "\n",
    "correct_reviews = correct_samples['review']\n",
    "misclassified_reviews = misclassified_samples['review']\n",
    "\n",
    "correct_lengths = list(map(lambda x: len(x), correct_reviews))\n",
    "misclass_lengths = list(map(lambda x: len(x), misclassified_reviews))\n",
    "\n",
    "correct_sentiment = [1 if x == 'positive' else 0 for x in correct_samples['sentiment']]\n",
    "misclass_sentiment = [1 if x == 'positive' else 0 for x in misclassified_samples['sentiment']]\n",
    "\n",
    "plt.hist(correct_lengths)\n",
    "plt.hist(misclass_lengths)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(correct_samples['sentiment'])\n",
    "plt.show()\n",
    "plt.hist(misclass_samples['sentiment'])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
