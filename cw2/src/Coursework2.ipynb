{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {},
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e0ac481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data_cache = {'Train_path':'', 'Test_path':'', 'X_train':None, 'y_train':None, 'X_test': None, 'y_train':None}\n",
    "\n",
    "def clear_cache():\n",
    "    data_cache['Test_path'] = ''\n",
    "    data_cache['Train_path'] = ''\n",
    "    data_cache['X_train'] = None\n",
    "    data_cache['y_train'] = None\n",
    "    data_cache['X_test'] = None\n",
    "    data_cache['y_test'] = None\n",
    "    \n",
    "def load_data(train_file, test_file):\n",
    "    # Read the CSV file and extract Bag of Words Features\n",
    "    \n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    raw_X_train = list(train_df.review)\n",
    "    y_train = list(map( (lambda x : 0 if x == 'negative' else 1), train_df.sentiment))\n",
    "    \n",
    "    raw_X_test = list(test_df.review)\n",
    "    y_test = list(map( (lambda x : 0 if x == 'negative' else 1), test_df.sentiment))\n",
    "    \n",
    "    return raw_train_x, train_y, raw_test_x, test_y \n",
    "\n",
    "# def clean_review(review):\n",
    "#     #review = word_tokenize(review)\n",
    "#     review = \"\".join([i for i in review if i not in string.punctuation])\n",
    "#     #stop_words = set(stopwords.words('english'))\n",
    "#     #review = [w for w in review if not w in stop_words]\n",
    "    \n",
    "#     return review\n",
    "\n",
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    if(data_cache['X_train'] is not None \n",
    "        and data_cache['y_train'] is not None\n",
    "        and data_cache['X_test']is not None\n",
    "        and data_cache['y_test'] is not None\n",
    "        and data_cache['Train_path'] == train_file\n",
    "        and data_cache['Test_path'] == test_file):\n",
    "        print('Getting from cache')\n",
    "        return (data_cache['X_train'], data_cache['y_train'], data_cache['X_test'], data_cache['y_test'])\n",
    "\n",
    "    # Read the CSV file and extract Bag of Words Features\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    raw_X_train = list(train_df.review)\n",
    "    raw_X_train = [clean_review(review) for review in raw_X_train]\n",
    "    y_train = list(map( (lambda x : 0 if x == 'negative' else 1), train_df.sentiment))\n",
    "    \n",
    "    raw_X_test = list(test_df.review)\n",
    "    raw_X_test = [clean_review(review) for review in raw_X_test]\n",
    "    y_test = list(map( (lambda x : 0 if x == 'negative' else 1), test_df.sentiment))\n",
    "    \n",
    "    X_train = get_embeddings(raw_X_train)\n",
    "    X_test  = get_embeddings(raw_X_test)\n",
    "    \n",
    "    data_cache['Test_path'] = test_file\n",
    "    data_cache['Train_path'] = train_file\n",
    "    data_cache['X_train'] = X_train\n",
    "    data_cache['y_train'] = y_train\n",
    "    data_cache['X_test'] = X_test\n",
    "    data_cache['y_test'] = y_test\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "def get_embeddings(reviews):\n",
    "    #\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedding_length = 128\n",
    "    embeddings = []\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "    model.max_seq_length = embedding_length\n",
    "    \n",
    "    for review in reviews:\n",
    "        if len(review.split(' ')) <= embedding_length:\n",
    "            embeddings.append(model.encode(review))\n",
    "        else:\n",
    "            ## If the review is longer than the maximum supported sequence length then embed in chunks and average\n",
    "            ## chunks overlap by 15 words to preserve some semantic continuity between chunks\n",
    "            split_review = review.split(' ')\n",
    "            split_review = [' '.join(split_review[i:i+embedding_length+15]) for i in range(0,len(split_review),embedding_length-15)]\n",
    "            split_embeddings = []\n",
    "            split_lens = []\n",
    "            for split in split_review:\n",
    "                split_lens.append(len(split))\n",
    "                embedding = model.encode(split)\n",
    "                split_embeddings.append(embedding)\n",
    "            embeddings.append(np.average(np.array(split_embeddings), axis = 0, weights = split_lens))\n",
    "                \n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "train_file = 'movie_review_test.csv'\n",
    "test_file =  'movie_review_test.csv'\n",
    "\n",
    "#x_test, y_train, x_test, y_test = extract_bag_of_words_train_test(train_file, test_file)\n",
    "#embeddings = get_embeddings(x)\n",
    "\n",
    "# print(len(x), len(y))\n",
    "# print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02ff5be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n",
      "[2743  273]\n",
      "2\n",
      "(2, 384)\n",
      "[3105  555]\n",
      "2\n",
      "(2, 384)\n",
      "[3028  850]\n",
      "2\n",
      "(2, 384)\n",
      "[2787 1033]\n",
      "2\n",
      "(2, 384)\n",
      "[2734  522]\n",
      "2\n",
      "(2, 384)\n",
      "[2754  483]\n",
      "2\n",
      "(2, 384)\n",
      "[2737   55]\n",
      "2\n",
      "(2, 384)\n",
      "[2721 1321]\n",
      "2\n",
      "(2, 384)\n",
      "[2672  279]\n",
      "2\n",
      "(2, 384)\n",
      "[3105 1992]\n",
      "2\n",
      "(2, 384)\n",
      "[2850  606]\n",
      "2\n",
      "(2, 384)\n",
      "[3004 1498]\n",
      "2\n",
      "(2, 384)\n",
      "[2878  191]\n",
      "2\n",
      "(2, 384)\n",
      "[2796  107]\n",
      "2\n",
      "(2, 384)\n",
      "[2771   70]\n",
      "2\n",
      "(2, 384)\n",
      "[2876 1495]\n",
      "2\n",
      "(2, 384)\n",
      "[2708  268]\n",
      "2\n",
      "(2, 384)\n",
      "[2995  737]\n",
      "2\n",
      "(2, 384)\n",
      "[2738 2143]\n",
      "2\n",
      "(2, 384)\n",
      "[2889  236]\n",
      "2\n",
      "(2, 384)\n",
      "[2828  772]\n",
      "2\n",
      "(2, 384)\n",
      "[2765  365]\n",
      "2\n",
      "(2, 384)\n",
      "[2764 2823]\n",
      "2\n",
      "(2, 384)\n",
      "[2905 2367]\n",
      "2\n",
      "(2, 384)\n",
      "[2788 1165]\n",
      "2\n",
      "(2, 384)\n",
      "[2780  344]\n",
      "2\n",
      "(2, 384)\n",
      "[2904  928]\n",
      "2\n",
      "(2, 384)\n",
      "[2779 1501]\n",
      "2\n",
      "(2, 384)\n",
      "[2761  238]\n",
      "2\n",
      "(2, 384)\n",
      "[2804 2662]\n",
      "2\n",
      "(2, 384)\n",
      "[2874 1101]\n",
      "2\n",
      "(2, 384)\n",
      "[3053 1401]\n",
      "2\n",
      "(2, 384)\n",
      "[2578  926]\n",
      "2\n",
      "(2, 384)\n",
      "[2593  631]\n",
      "2\n",
      "(2, 384)\n",
      "[2865   50]\n",
      "2\n",
      "(2, 384)\n",
      "[2926 1161]\n",
      "2\n",
      "(2, 384)\n",
      "[2708   47]\n",
      "2\n",
      "(2, 384)\n",
      "[2852  686]\n",
      "2\n",
      "(2, 384)\n",
      "[2863  778]\n",
      "2\n",
      "(2, 384)\n",
      "[2760  821]\n",
      "2\n",
      "(2, 384)\n",
      "[2739  901]\n",
      "2\n",
      "(2, 384)\n",
      "[2821 2695]\n",
      "2\n",
      "(2, 384)\n",
      "[2712 2013]\n",
      "2\n",
      "(2, 384)\n",
      "[2849 1862]\n",
      "2\n",
      "(3, 384)\n",
      "[2727 2755 1427]\n",
      "3\n",
      "(2, 384)\n",
      "[2722  349]\n",
      "2\n",
      "(2, 384)\n",
      "[2739 1687]\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, y_train, X_test, y_test  \u001b[38;5;241m=\u001b[39m \u001b[43mextract_bag_of_words_train_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovie_review_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovie_review_test.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36mextract_bag_of_words_train_test\u001b[0;34m(train_file, test_file)\u001b[0m\n\u001b[1;32m     60\u001b[0m raw_X_test \u001b[38;5;241m=\u001b[39m [clean_review(review) \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m raw_X_test]\n\u001b[1;32m     61\u001b[0m y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m( (\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m), test_df\u001b[38;5;241m.\u001b[39msentiment))\n\u001b[0;32m---> 63\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_X_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m X_test  \u001b[38;5;241m=\u001b[39m get_embeddings(raw_X_test)\n\u001b[1;32m     66\u001b[0m data_cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_file\n",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m split_review:\n\u001b[1;32m     91\u001b[0m     split_lens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(split))\n\u001b[0;32m---> 92\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     split_embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(split_embeddings)\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:164\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    161\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 164\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    167\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    995\u001b[0m )\n\u001b[0;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:402\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    394\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 402\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    412\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:290\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    288\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    291\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    293\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50265-r1L_TYHD/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test  = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68b1dc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')#'sentence-transformers/paraphrase-MiniLM-L3-v2')\n",
    "model.max_seq_length = 1024\n",
    "model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e94c07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #implement initialisation\n",
    "        #self.some_paramter=1\n",
    "        self.clf = svm.SVC(decision_function_shape='ovo')\n",
    "        self.clf.kernel = 'rbf'\n",
    "        \n",
    "    # define your own kernel here\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html\n",
    "    def fit(self, X,y):\n",
    "        # training of the SVM\n",
    "        # Ensure you call your own defined kernel here\n",
    "\n",
    "        self.clf.fit(X, y)\n",
    "               \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        predictions = self.clf.predict(X)\n",
    "        #print('predictions:', len(predictions))\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89603f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test_func_svm(dataset_train, dataset_test):    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    #print(X_train, Y_train)\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    #print(\"Accuracy:\",acc)\n",
    "    return acc\n",
    "\n",
    "def test_svm(model, train_x, train_y, test_x, test_y):  \n",
    "    sc = model \n",
    "    #print(X_train, Y_train)\n",
    "    sc.fit(train_x, train_y)\n",
    "    Y_Pred = sc.predict(test_x)\n",
    "    acc = accuracy_score(test_y, Y_Pred)\n",
    "    #print(\"Accuracy:\",acc)\n",
    "    return acc\n",
    "\n",
    "def crossval_svm(model, x, y, kfolds = 5):\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    kfold_accuracys = []\n",
    "    \n",
    "    kf = KFold(n_splits = kfolds)\n",
    "    train_x = kf.split(x,y)\n",
    "    #print(train_x)\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in kf.split(x):\n",
    "        train_x = x[train_index]\n",
    "        train_y = y[train_index]\n",
    "        val_x = x[val_index]\n",
    "        val_y = y[val_index]\n",
    "        \n",
    "        acc = test_svm(model, train_x, train_y, val_x, val_y)\n",
    "        #print(acc)\n",
    "        kfold_accuracys.append(acc)\n",
    "        \n",
    "    return np.mean(kfold_accuracys)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15ac1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test  = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ffd4adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/daniel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting from cache\n",
      "Cross val accuracy:  0.5138 Test accuracy:  0.528\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "model = SVMClassifier()\n",
    "#Hyperparams chosen with grid search, need to implement params in the wrapper constructor\n",
    "#model.clf = svm.SVC(decision_function_shape='ovo', kernel = 'rbf', gamma = 1, C = 1)\n",
    "\n",
    "model.clf = svm.SVC(decision_function_shape='ovo', kernel = 'poly', degree = 1, C = 1)\n",
    "cross_val_acc = crossval_svm(model, X_train, np.array(y_train))\n",
    "\n",
    "test_acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "\n",
    "print(\"Cross val accuracy: \", cross_val_acc, \"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12d08be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5488.145039273248, 'gamma': 7151.903663724195}\n"
     ]
    }
   ],
   "source": [
    "### rbf hyperparam tuning - very slow do not run - it resulted in c=1, gamma=1\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "\n",
    "C_range = 10. ** np.linspace(-3, 3, 15)\n",
    "gamma_range = 10. ** np.linspace(-3,4, 15)\n",
    "distributions = dict(C = uniform(loc=10e-3, scale=10e3), gamma = uniform(loc=10e-3, scale=10e3))\n",
    "\n",
    "#hyperparameters = {'C':C_range, 'gamma':gamma_range}\n",
    "svc = SVMClassifier()\n",
    "tuner = RandomizedSearchCV(svm.SVC(), distributions, scoring='accuracy', n_iter=20, random_state=0)\n",
    "tuner.fit(X_train, y_train)\n",
    "\n",
    "print(tuner.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb2d81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'degree': 1.0, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "### polynomial hyperparam tuning - it chose 1, 1 again\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "\n",
    "C_range = 10. ** np.linspace(-3, 3, 15)\n",
    "degree_range = np.linspace(1, 20, 6)\n",
    "\n",
    "hyperparameters = {'kernel': ['poly'], 'C':C_range, 'degree':degree_range}\n",
    "svc = SVMClassifier()\n",
    "tuner = GridSearchCV(svc.clf, hyperparameters,scoring = 'accuracy', cv=StratifiedKFold(5))\n",
    "tuner.fit(X_train, y_train)\n",
    "\n",
    "print(tuner.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3805e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    def __init__(self, num_classifiers = 50, max_depth = 3):\n",
    "        import numpy as np\n",
    "        #implement initialisation\n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.max_depth = max_depth\n",
    "        self.classifiers = []\n",
    "        self.alphas = np.ones(self.num_classifiers)\n",
    "        self.alphas /= self.num_classifiers\n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        signed_y = [1 if x == 1 else -1 for x in y]\n",
    "        num_samples = X.shape[0]\n",
    "        w = np.random.rand(num_samples)\n",
    "        w /= num_samples\n",
    "        alph = np.ones(self.num_classifiers)\n",
    "        #print(signed_y)\n",
    "        for c in range(self.num_classifiers):\n",
    "#             plt.hist(w)\n",
    "#             plt.show()\n",
    "            \n",
    "            clf = tree.DecisionTreeClassifier(max_depth = self.max_depth)#splitter='random')\n",
    "            self.classifiers.append(clf)\n",
    "            \n",
    "#             random_w = np.random.rand(num_samples)\n",
    "#             random_w /= np.sum(random_w)\n",
    "            clf.fit(X, signed_y, sample_weight = w)\n",
    "            \n",
    "            #get predictions and calculate error rate\n",
    "            predictions = clf.predict(X)     \n",
    "            error_rate = np.count_nonzero(predictions != signed_y) / num_samples\n",
    "            #print(error_rate)\n",
    "            #print('training:', h[:5])\n",
    "            #set classifier weight\n",
    "            alpha = 0.5 * math.log((1-error_rate)/error_rate) if error_rate > 0 else 1\n",
    "            self.alphas[c]  = alpha\n",
    "            #update sample weights\n",
    "            \n",
    "            new_w = np.copy(w)\n",
    "            for i in range(num_samples):\n",
    "                w[i] = w[i] * math.exp(-1 * alpha * signed_y[i] * predictions[i])\n",
    "                \n",
    "            w /= np.sum(new_w)\n",
    "        \n",
    "#         plt.hist(w)\n",
    "#         plt.show() \n",
    "#         plt.hist(self.alphas)\n",
    "#         plt.show()\n",
    "        #implement training of the boosting classifier\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep= False):\n",
    "        return {'num_classifiers':self.num_classifiers, 'max_depth':self.max_depth}\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # implement prediction of the boosting classifier\n",
    "        h = []\n",
    "        accum = []\n",
    "        for x in X:\n",
    "            cumsum = 0\n",
    "            for c in range(self.num_classifiers):\n",
    "                cumsum += self.alphas[c] * self.classifiers[c].predict(np.array(x).reshape(1,-1))\n",
    "                accum.append(cumsum)\n",
    "                #print(cumsum)\n",
    "            h.append(np.sign(cumsum))\n",
    "        #print('predicting:', h[:5])\n",
    "        return [0 if x < 0 else 1 for x in h]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4632591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    X_train, Y_train, X_test, Y_test = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    #print(X_test.shape)\n",
    "    Y_Pred = bc.predict(X_test)\n",
    "    #print(np.array(Y_test).shape, np.array(Y_Pred).shape)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6c27de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.7746666666666666\n"
     ]
    }
   ],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca42a29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score  \n",
    "def crossval_boost(model, x, y, kfolds = 5):\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    kfold_accuracys = []\n",
    "    \n",
    "    kf = KFold(n_splits = kfolds)\n",
    "    train_x = kf.split(x,y)\n",
    "    #print(train_x)\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in kf.split(x):\n",
    "        #print(train_index, val_index)\n",
    "        train_x = x[train_index]\n",
    "        train_y = y[train_index]\n",
    "        val_x = x[val_index]\n",
    "        val_y = y[val_index]\n",
    "        \n",
    "        bc = BoostingClassifier()\n",
    "        bc.fit(train_x, train_y)\n",
    "        Y_Pred = bc.predict(val_x)\n",
    "        acc = accuracy_score(val_y, Y_Pred)\n",
    "        \n",
    "        #print(acc)\n",
    "        kfold_accuracys.append(acc)\n",
    "        \n",
    "    return np.mean(kfold_accuracys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e714ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting from cache\n",
      "Acc: 0.7222000000000001 Params: 50 2\n",
      "Acc: 0.7292 Params: 50 5\n",
      "Acc: 0.7205999999999999 Params: 50 10\n",
      "Acc: 0.7216 Params: 50 25\n",
      "Acc: 0.7228 Params: 100 2\n",
      "Acc: 0.7262000000000001 Params: 100 5\n",
      "Acc: 0.7272 Params: 100 10\n",
      "Acc: 0.7302 Params: 100 25\n",
      "Acc: 0.7212 Params: 200 2\n",
      "Acc: 0.7304 Params: 200 5\n",
      "Acc: 0.7172 Params: 200 10\n",
      "Acc: 0.724 Params: 200 25\n",
      "Acc: 0.6968 Params: 500 2\n",
      "Acc: 0.727 Params: 500 5\n",
      "Acc: 0.7260000000000001 Params: 500 10\n",
      "Acc: 0.7163999999999999 Params: 500 25\n",
      "Best Params: 200 5\n"
     ]
    }
   ],
   "source": [
    "## Hyperparam tuning for gradboost\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "#X_train, y_train, X_test, Y_test = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "\n",
    "\n",
    "def grid_search(params):\n",
    "    X_train, y_train, X_test, Y_test = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "    clfs = []\n",
    "    best_acc = 0\n",
    "\n",
    "    for num in params['num_classifiers']:\n",
    "        for max_depth in params['max_depth']:\n",
    "            clfs.append(BoostingClassifier(num, max_depth))\n",
    "    for clf in clfs:\n",
    "        accuracy = crossval_boost(clf, X_train, np.array(y_train), 5)\n",
    "        print('Acc:' , accuracy, 'Params:', clf.num_classifiers, clf.max_depth)\n",
    "        if accuracy > best_acc:\n",
    "            best_clf = clf\n",
    "            best_acc = accuracy\n",
    "            \n",
    "    print(\"Best Params:\", best_clf.num_classifiers, best_clf.max_depth)\n",
    "    return best_clf\n",
    "    \n",
    "grid = {'num_classifiers': [50, 100, 200, 500], 'max_depth': [2, 5, 10, 25]}\n",
    "best_clf = grid_search(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6200428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning = np.array([50, 100, 200, 500], [2, 5, 10, 25], )\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58afdc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review sentiment  Unnamed: 2\n",
      "0     Not a movie for everyone, but this movie is in...  positive         NaN\n",
      "1     This film is not your typical Hollywood fare, ...  positive         NaN\n",
      "2     RKO Radio Pictures made a real classic in 1947...  positive         NaN\n",
      "3     \"Transylvania 6-5000\" is an insignificant but ...  negative         NaN\n",
      "4     This is a very good, under-rated action/drama/...  positive         NaN\n",
      "...                                                 ...       ...         ...\n",
      "1495  The essential message - one which Miller would...  positive         NaN\n",
      "1496  This movie is tremendous for uplifting the Spi...  positive         NaN\n",
      "1497  I think this movie is different apart from mos...  positive         NaN\n",
      "1498  Brit director Chrstopher Nolan now has a caree...  positive         NaN\n",
      "1499  The Salena Incident is set in Arizona where si...  negative         NaN\n",
      "\n",
      "[1500 rows x 3 columns]\n",
      "                                                 review sentiment  Unnamed: 2\n",
      "1     This film is not your typical Hollywood fare, ...  positive         NaN\n",
      "4     This is a very good, under-rated action/drama/...  positive         NaN\n",
      "6     \"You got any beans? Beans is good. You just ea...  negative         NaN\n",
      "7     These immortal lines begin The Jack Starret di...  positive         NaN\n",
      "8     When this movie was first shown on television ...  negative         NaN\n",
      "...                                                 ...       ...         ...\n",
      "1493  Steven Seagal, Mr. Personality himself, this t...  negative         NaN\n",
      "1494  I am new at this, so bear with me please. I am...  positive         NaN\n",
      "1496  This movie is tremendous for uplifting the Spi...  positive         NaN\n",
      "1498  Brit director Chrstopher Nolan now has a caree...  positive         NaN\n",
      "1499  The Salena Incident is set in Arizona where si...  negative         NaN\n",
      "\n",
      "[1116 rows x 3 columns]\n",
      "                                                 review sentiment  Unnamed: 2\n",
      "0     Not a movie for everyone, but this movie is in...  positive         NaN\n",
      "2     RKO Radio Pictures made a real classic in 1947...  positive         NaN\n",
      "3     \"Transylvania 6-5000\" is an insignificant but ...  negative         NaN\n",
      "5     I almost made a fool of myself when I was goin...  negative         NaN\n",
      "14    Flashes of lightning; a sprawling cemetery; th...  positive         NaN\n",
      "...                                                 ...       ...         ...\n",
      "1485  You can't watch this film for a history lesson...  positive         NaN\n",
      "1486  This movie was just as good as some of the oth...  positive         NaN\n",
      "1492  I just watched this movie on Showtime. Quite b...  positive         NaN\n",
      "1495  The essential message - one which Miller would...  positive         NaN\n",
      "1497  I think this movie is different apart from mos...  positive         NaN\n",
      "\n",
      "[384 rows x 3 columns]\n",
      "[96, 144, 361, 131, 461, 374, 544, 147, 72, 89, 208, 217, 563, 174, 174, 134, 184, 84, 207, 314, 144, 232, 51, 226, 295, 76, 163, 101, 276, 206, 226, 189, 117, 510, 122, 195, 126, 737, 135, 316, 234, 192, 114, 937, 135, 71, 160, 148, 135, 129, 321, 150, 136, 127, 112, 304, 130, 138, 163, 124, 214, 406, 112, 44, 447, 219, 57, 120, 213, 77, 449, 238, 108, 240, 314, 254, 79, 212, 162, 447, 120, 121, 187, 303, 245, 355, 266, 333, 410, 133, 251, 566, 185, 854, 605, 323, 100, 316, 103, 104, 415, 62, 156, 139, 65, 138, 310, 153, 73, 142, 697, 144, 150, 29, 203, 358, 173, 388, 125, 179, 150, 613, 118, 295, 145, 326, 168, 71, 524, 253, 389, 127, 192, 57, 739, 120, 138, 204, 532, 281, 291, 213, 320, 118, 521, 130, 129, 165, 149, 124, 204, 134, 136, 142, 230, 144, 161, 590, 445, 643, 47, 43, 276, 88, 43, 267, 92, 91, 150, 224, 175, 141, 62, 112, 138, 101, 723, 294, 543, 196, 421, 119, 512, 107, 103, 129, 154, 138, 335, 540, 201, 114, 188, 461, 279, 735, 117, 374, 123, 318, 200, 83, 197, 113, 768, 137, 190, 136, 127, 524, 750, 226, 584, 548, 161, 266, 195, 158, 39, 148, 205, 185, 54, 271, 42, 606, 223, 147, 151, 163, 113, 127, 166, 356, 155, 535, 166, 294, 179, 106, 143, 109, 302, 201, 176, 482, 142, 272, 403, 253, 156, 60, 227, 116, 234, 116, 151, 967, 210, 123, 124, 217, 176, 150, 805, 104, 368, 61, 162, 740, 957, 70, 626, 273, 124, 198, 141, 141, 216, 139, 251, 132, 107, 60, 131, 325, 149, 211, 160, 137, 123, 72, 358, 131, 462, 953, 120, 208, 113, 200, 245, 137, 56, 177, 60, 147, 211, 108, 112, 286, 590, 193, 127, 144, 228, 573, 228, 293, 152, 84, 171, 142, 51, 132, 155, 173, 261, 384, 249, 410, 92, 34, 113, 929, 534, 275, 57, 699, 879, 142, 106, 263, 377, 195, 102, 43, 156, 131, 81, 335, 158, 142, 203, 227, 201, 366, 208, 421, 220, 242, 241, 147, 53, 218, 144, 162, 129, 238, 208, 152, 155, 335, 106, 490, 107, 149, 138, 161, 71, 119, 366, 306, 110, 131, 412, 392, 119, 80, 332, 135, 184, 122, 148, 188, 372, 156, 245, 801, 385, 132, 68, 71, 120, 101, 332, 162, 125, 565, 329, 155, 105, 696, 171, 200, 83, 146, 364, 116, 127, 140, 317, 517, 118, 120, 237, 147, 208, 269, 191, 70, 425, 65, 239, 303, 59, 182, 269, 112, 300, 775, 259, 195, 700, 272, 469, 204, 142, 314, 177, 150, 129, 197, 452, 487, 196, 151, 126, 62, 261, 160, 206, 115, 190, 160, 204, 114, 215, 127, 144, 963, 598, 133, 169, 160, 198, 155, 87, 113, 88, 281, 88, 354, 156, 285, 425, 182, 162, 111, 188, 213, 883, 988, 343, 825, 145, 139, 773, 48, 48, 418, 302, 317, 104, 135, 141, 170, 106, 380, 248, 145, 126, 100, 408, 254, 149, 63, 172, 130, 312, 109, 105, 263, 222, 985, 320, 198, 110, 173, 178, 477, 207, 261, 65, 171, 113, 121, 239, 210, 140, 63, 197, 636, 162, 179, 567, 95, 95, 506, 279, 290, 533, 111, 159, 893, 526, 701, 184, 278, 156, 168, 219, 127, 208, 296, 135, 349, 303, 145, 78, 75, 78, 100, 257, 115, 41, 139, 477, 132, 170, 222, 204, 151, 119, 228, 156, 156, 218, 311, 403, 134, 92, 279, 63, 199, 117, 213, 176, 201, 140, 194, 374, 136, 130, 178, 785, 931, 167, 96, 734, 309, 167, 306, 59, 75, 403, 121, 67, 703, 217, 358, 148, 459, 111, 51, 131, 326, 294, 123, 296, 117, 235, 154, 223, 133, 196, 109, 149, 179, 243, 214, 55, 464, 70, 116, 278, 617, 117, 172, 322, 136, 142, 602, 560, 666, 74, 235, 131, 190, 294, 292, 160, 226, 444, 120, 530, 178, 77, 203, 159, 176, 123, 116, 189, 551, 824, 36, 143, 406, 101, 844, 168, 731, 239, 156, 237, 563, 178, 343, 263, 511, 156, 301, 115, 487, 118, 420, 390, 126, 210, 322, 131, 78, 667, 316, 44, 238, 146, 115, 436, 649, 174, 51, 583, 197, 248, 366, 112, 101, 360, 188, 557, 635, 51, 124, 132, 243, 467, 99, 354, 200, 224, 105, 182, 120, 339, 160, 122, 588, 168, 289, 291, 141, 657, 191, 129, 115, 120, 122, 274, 54, 195, 137, 634, 777, 127, 120, 137, 388, 211, 132, 109, 199, 679, 785, 164, 253, 42, 174, 229, 165, 51, 423, 172, 332, 119, 126, 101, 192, 151, 334, 93, 49, 154, 156, 407, 133, 418, 174, 147, 69, 138, 223, 41, 274, 266, 253, 143, 134, 82, 163, 822, 106, 121, 177, 94, 160, 145, 103, 69, 368, 128, 147, 550, 146, 247, 214, 194, 635, 763, 118, 276, 137, 220, 137, 160, 56, 299, 201, 174, 160, 113, 168, 118, 249, 116, 132, 188, 128, 607, 88, 143, 155, 102, 566, 519, 88, 149, 154, 116, 183, 227, 217, 982, 181, 182, 838, 145, 328, 442, 289, 155, 205, 193, 219, 139, 161, 134, 133, 167, 227, 305, 121, 206, 110, 113, 117, 134, 200, 240, 203, 120, 222, 430, 66, 572, 174, 66, 145, 96, 429, 27, 164, 201, 365, 84, 122, 487, 113, 138, 93, 365, 335, 140, 98, 110, 813, 152, 153, 210, 376, 48, 132, 113, 132, 49, 155, 153, 133, 79, 54, 131, 1001, 152, 267, 122, 103, 485, 55, 118, 179, 119, 272, 128, 462, 111, 284, 86, 262, 298, 631, 147, 142, 72, 135, 170, 151, 171, 235, 294, 582, 153, 137, 125, 119, 119, 129, 152, 125, 123, 200, 115, 113, 145, 386, 212, 408, 247, 226, 73, 60, 174, 282, 179, 531, 87, 125, 100, 302, 187, 129, 77, 435, 307, 163, 174, 131, 853, 76, 293, 185, 113, 162, 155, 76, 184, 47, 68, 219, 124, 196, 178, 225, 397, 118, 139, 128, 81, 126, 47, 100, 169, 192, 105, 178, 649, 61, 140, 115, 48, 279, 393, 135, 262, 130, 294, 190, 305, 126, 176, 223, 147, 190, 254, 320, 302, 314, 94, 449, 62, 575, 199, 152, 272, 270, 77, 64, 1001, 487, 154, 268, 731, 232, 120, 194, 160, 135, 309, 53, 136, 115, 145, 128, 159, 184, 388, 85, 130, 45, 115, 306, 159, 136, 134, 49, 155, 136, 119, 308, 121, 144, 130, 149, 158, 111, 118, 270, 126, 128, 200, 155, 127, 174, 189, 302, 249, 216, 115, 204, 152, 544, 69, 485, 446, 158, 316, 208, 177, 334, 128, 267, 290, 388, 200, 311, 134, 563]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, y_train, X_test, Y_test = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "raw_X_test = pd.read_csv(\"movie_review_test.csv\")\n",
    "\n",
    "\n",
    "bc = BoostingClassifier()\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "Y_Pred = bc.predict(X_test)\n",
    "print(raw_X_test)\n",
    "\n",
    "correct_indices = np.nonzero(np.equal(Y_Pred, Y_test))\n",
    "misclassified_indices = np.nonzero(np.not_equal(Y_Pred, Y_test))\n",
    "\n",
    "correct_samples = raw_X_test.iloc[correct_indices]\n",
    "misclassified_samples = raw_X_test.iloc[misclassified_indices]\n",
    "\n",
    "print(correct_samples)\n",
    "print(misclassified_samples)\n",
    "\n",
    "correct_reviews = correct_samples['review']\n",
    "misclassified_reviews = misclassified_samples['review']\n",
    "\n",
    "correct_lengths = list(map(lambda x: len(x.split(' ')), correct_reviews))\n",
    "misclass_lengths = list(map(lambda x: len(x.split(' ')), misclassified_reviews))\n",
    "\n",
    "correct_sentiment = [1 if x == 'positive' else 0 for x in correct_samples['sentiment']]\n",
    "misclass_sentiment = [1 if x == 'positive' else 0 for x in misclassified_samples['sentiment']]\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(correct_lengths)\n",
    "plt.hist(misclass_lengths)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(correct_samples['sentiment'])\n",
    "plt.show()\n",
    "plt.hist(misclassified_samples['sentiment'])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
