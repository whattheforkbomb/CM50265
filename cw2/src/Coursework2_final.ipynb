{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44281064",
   "metadata": {},
   "source": [
    "### Coursework 2\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. \n",
    "Both the classification tasks relate to text classification tasks. \n",
    "\n",
    "One task is to be solved using Support Vector Machines. The other has to be solved using Boosting.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single zip file. You could have additional functions implemented that you require for carrying out each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffe46",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train an SVM based classifier to obtain train and check on the sample test dataset provided. The method will be evaluated also against an external test set. Please do not hardcode any dimensions or number of samples while writing the code. It should be possible to automate the testing and hardcoding values does not allow for automated testing. \n",
    "\n",
    "You are allowed to use scikit-learn to implement the SVM. However, you are expected to write your own kernels.\n",
    "\n",
    "You are allowed to use the existing library functions such as scikit-learn or numpy for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. Refer to the documentation provided [here](https://scikit-learn.org/stable/modules/svm.html) at 1.4.6.2 and an example [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html) for writing your own kernels.\n",
    "\n",
    "Details regarding the marking have been provided in the coursework specification file. Ensure that the code can be run with different test files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ce53",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ac481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "data_cache = {'Train_path':'', 'Test_path':'', 'X_train':None, 'y_train':None, 'X_test': None, 'y_train':None}\n",
    "\n",
    "def clear_cache():\n",
    "    data_cache['Test_path'] = ''\n",
    "    data_cache['Train_path'] = ''\n",
    "    data_cache['X_train'] = None\n",
    "    data_cache['y_train'] = None\n",
    "    data_cache['X_test'] = None\n",
    "    data_cache['y_test'] = None\n",
    "\n",
    "def load_data(train_file, test_file):\n",
    "    # Read the CSV file and extract Bag of Words Features\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    raw_train_x = list(train_df.review)\n",
    "    train_y = list(map((lambda x : 0 if x == 'negative' else 1), train_df.sentiment))\n",
    "    \n",
    "    raw_test_x = list(test_df.review)\n",
    "    test_y = list(map((lambda x : 0 if x == 'negative' else 1), test_df.sentiment))\n",
    "    \n",
    "    return raw_train_x, np.array(train_y), raw_test_x, np.array(test_y)\n",
    "\n",
    "def extract_bag_of_words_train_test(train_file, test_file):\n",
    "    if(data_cache['X_train'] is not None \n",
    "        and data_cache['y_train'] is not None\n",
    "        and data_cache['X_test']is not None\n",
    "        and data_cache['y_test'] is not None\n",
    "        and data_cache['Train_path'] == train_file\n",
    "        and data_cache['Test_path'] == test_file):\n",
    "        print('Getting from cache')\n",
    "        return (data_cache['X_train'], data_cache['y_train'], data_cache['X_test'], data_cache['y_test'])\n",
    "    \n",
    "    raw_train_x, y_train, raw_test_x, y_test = load_data(train_file, test_file)\n",
    "    \n",
    "    X_train = get_embeddings(raw_train_x)\n",
    "    X_test  = get_embeddings(raw_test_x)\n",
    "    data_cache['Test_path'] = test_file\n",
    "    data_cache['Train_path'] = train_file\n",
    "    data_cache['X_train'] = X_train\n",
    "    data_cache['y_train'] = y_train\n",
    "    data_cache['X_test'] = X_test\n",
    "    data_cache['y_test'] = y_test\n",
    "    return (train_x, train_y, test_x, test_y)\n",
    "\n",
    "def get_embeddings(reviews):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedding_length = 128\n",
    "    embeddings = []\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "    model.max_seq_length = embedding_length\n",
    "    \n",
    "    for review in reviews:\n",
    "        if len(review.split(' ')) <= embedding_length:\n",
    "            embeddings.append(model.encode(review))\n",
    "        else:\n",
    "            ## If the review is longer than the maximum supported sequence length then embed in chunks and average\n",
    "            ## chunks overlap by 15 words to preserve some semantic continuity between chunks\n",
    "            split_review = review.split(' ')\n",
    "            split_review = [' '.join(split_review[i:i+embedding_length+15]) for i in range(0,len(split_review),embedding_length-15)]\n",
    "            split_embeddings = []\n",
    "            split_lens = []\n",
    "            for split in split_review:\n",
    "                split_lens.append(len(split))\n",
    "                embedding = model.encode(split)\n",
    "                split_embeddings.append(embedding)\n",
    "            embeddings.append(np.average(np.array(split_embeddings), axis = 0, weights = split_lens))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94c07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import itertools\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #implement initialisation\n",
    "        #self.some_paramter=1\n",
    "        self.clf = svm.SVC(decision_function_shape='ovr', C=0.9,  gamma=2.2857142857142856)\n",
    "        self.clf.kernel = 'rbf'\n",
    "        \n",
    "    # define your own kernel here\n",
    "    # Refer to the documentation here: https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html\n",
    "    def fit(self, X, y):\n",
    "        # training of the SVM\n",
    "        # Ensure you call your own defined kernel here\n",
    "        # defined below as own method to permit hyperparam tuning via builder method\n",
    "        self.clf.fit(X, y)\n",
    "               \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        predictions = self.clf.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "def linear_kernel_builder(M):\n",
    "    # effectively multiply each feature by some weights and dot product with expected output\n",
    "    def linear_kernel(X, Y): \n",
    "        weighted_features = (X * M.T)\n",
    "        out = weighted_features @ Y.T\n",
    "        return out\n",
    "    return linear_kernel\n",
    "    \n",
    "def cross_validation(model, params, X_train, Y_train, kfolds):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    k_folder = KFold(n_splits=kfolds, shuffle=True)\n",
    "\n",
    "    def test_svm(train_x, train_y, test_x, test_y):\n",
    "        model.fit(train_x, train_y)\n",
    "        Y_Pred = model.predict(test_x)\n",
    "        return accuracy_score(test_y, Y_Pred)\n",
    "\n",
    "    accuracy = []\n",
    "    for train_index, val_index in k_folder.split(X_train):\n",
    "        train_x = X_train[train_index, :]\n",
    "        train_y = Y_train[train_index]\n",
    "        val_x = X_train[val_index, :]\n",
    "        val_y = Y_train[val_index]\n",
    "\n",
    "        acc = test_svm(train_x, train_y, val_x, val_y)\n",
    "        accuracy.append(acc)\n",
    "\n",
    "    return (params, np.mean(accuracy))\n",
    "    \n",
    "# Find best hyper-params via grid search with k-fold cross-validation\n",
    "def hyperparam_tuning(model_builder, search_space, X_train, Y_train, kfolds=5):\n",
    "    import multiprocessing as mp\n",
    "    \n",
    "    cross_val_history = [] # (params, mean_acc)\n",
    "    count = 0\n",
    "    for params in itertools.product(*search_space):\n",
    "        # if count == 0:\n",
    "        #     print(f\"Processing {params}\") # just ensuring some output is provided to give best \n",
    "        # count += 1\n",
    "        # if count == 20:\n",
    "        #     count = 0\n",
    "        cross_val_history.append(cross_validation(model_builder(params), params, X_train, Y_train, kfolds))\n",
    "    # Didn't really see any speed-up with parallelisation, seemed to prevent models themselves from using threading...\n",
    "    # num_cores = max(1, int(mp.cpu_count() / 2)) # don't hog machine resources\n",
    "    # with mp.Pool(processes=num_cores) as pool:\n",
    "        # cross_val_history = pool.starmap(cross_validation, ((model_builder(params), params, X_train, Y_train, kfolds) for params in itertools.product(*search_space)))\n",
    "    \n",
    "    best_params = max(cross_val_history, key=lambda run: run[1])\n",
    "    \n",
    "    return best_params, cross_val_history\n",
    "\n",
    "## SVM Evaluation\n",
    "def svm_analysis(X_train, Y_train, X_test, Y_test):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Get best hyper-params for each kernel via k-fold cross-validation\n",
    "    # Regularisation coefficient (for all kernels), must be >0, and given regularisation, can be kept below 1\n",
    "    regularisation_range = np.linspace(2e-1, 1, 9) # C\n",
    "    \n",
    "    # Kernel coefficient (for rbf and poly kernels)\n",
    "    gamma_range = np.linspace(0.5, 3, 8) # gamma\n",
    "    \n",
    "    # RBF kernel\n",
    "    def rbf_model_builder(params): \n",
    "        model = SVMClassifier()\n",
    "        model.clf.kernel = 'rbf'\n",
    "        model.clf.set_params(**{'C': params[0], 'gamma': params[1]})\n",
    "        return model\n",
    "    \n",
    "    # Poly Kernel\n",
    "    # Number of terms for the polynomial kernel (must be > 0)\n",
    "    degree_range = np.linspace(1, 3, 3)\n",
    "    def poly_model_builder(params): \n",
    "        model = SVMClassifier()\n",
    "        model.clf.kernel = 'poly'\n",
    "        model.clf.set_params(**{'C': params[0], 'gamma': params[1], 'degree': params[2]})\n",
    "        return model\n",
    "    \n",
    "    # Linear Kernel\n",
    "    # analyse features of embedded training data to determine most relevent features to reduce search space\n",
    "    embeddings = pd.DataFrame({idx: X_train[:, idx] for idx in range(X_train.shape[1])})\n",
    "    embeddings['y'] = Y_train\n",
    "    feature_correlation = embeddings.corr()['y'][:].drop('y')\n",
    "    # get top 3 positive and negative correlated features\n",
    "    positive_correlations = feature_correlation.nlargest(3)\n",
    "    negative_correlations = feature_correlation.nsmallest(3)\n",
    "    correlations_idx = np.concatenate((positive_correlations.index.values, negative_correlations.index.values))\n",
    "    print(f\"Top 3 positively correlated features:\\n{positive_correlations}\")\n",
    "    print(f\"Top 3 negatively correlated features:\\n{negative_correlations}\")\n",
    "    \n",
    "    # Generate correlation heatmap\n",
    "    fig, axs = plt.subplots(2, 12, sharex=True, figsize=(10, 6))\n",
    "    for x_idx, y_idx in itertools.product(*[range(2), range(12)]):\n",
    "        corr_idx = ((12 * x_idx) + y_idx) * 16\n",
    "        feature_slice = feature_correlation.iloc[corr_idx:corr_idx+16]\n",
    "        ax = axs[x_idx, y_idx]\n",
    "        im = ax.imshow(feature_slice[:, np.newaxis], vmin=-0.3, vmax=0.3)\n",
    "        ax.set_yticks(range(16), feature_slice.index.values + 1)\n",
    "        ax.set_xticks([])\n",
    "    \n",
    "    fig.suptitle('Heatmap Showing Correlation Between Embedded Feature and Review Sentiment')\n",
    "    fig.supylabel(\"Feature Number\")\n",
    "    cbar_ax = fig.add_axes([0.9, 0.15, 0.05, 0.7])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.set_label(\"Correlation\", rotation=270)\n",
    "    plt.show()\n",
    "    \n",
    "    # Range within which to draw possible values from weights for the linear kernel\n",
    "    weight_range = [np.linspace(1, 5, 3) for _ in range(6)]\n",
    "    def linear_model_builder(params): \n",
    "        model = SVMClassifier()\n",
    "        # create a vector that is ones, except for the most prominent features, which we want to increase the weight of\n",
    "        M = np.ones(X_train.shape[1])\n",
    "        for m_idx, p_idx in zip(correlations_idx, range(1, len(params))):\n",
    "            M[m_idx] = params[p_idx]\n",
    "        kernel = linear_kernel_builder(M)\n",
    "        model.clf.kernel = kernel\n",
    "        model.clf.set_params(**{'C': params[0]})\n",
    "        return model\n",
    "    \n",
    "    import timeit\n",
    "    print(\"Performing RBF Kernel Hyper-Parameter Optimisation\")\n",
    "    start = timeit.timeit()\n",
    "    rbf_hyperparams, rbf_val_history = hyperparam_tuning(rbf_model_builder, [regularisation_range, gamma_range], X_train, Y_train)\n",
    "    end = timeit.timeit()\n",
    "    print(f\"RBF tuning complete, time elapsed: {end-start}\")\n",
    "    \n",
    "    print(\"Performing Polynomial Kernel Hyper-Parameter Optimisation\")\n",
    "    poly_start = timeit.timeit()\n",
    "    poly_hyperparams, poly_val_history = hyperparam_tuning(poly_model_builder, [regularisation_range, gamma_range, degree_range], X_train, Y_train)\n",
    "    end = timeit.timeit()\n",
    "    print(f\"Polynomial tuning complete, time elapsed: {end-poly_start}\")\n",
    "    \n",
    "    print(\"Performing Linear Kernel Hyper-Parameter Optimisation\")\n",
    "    linear_start = timeit.timeit()\n",
    "    linear_hyperparams, linear_val_history = hyperparam_tuning(linear_model_builder, [regularisation_range] + weight_range, X_train, Y_train)\n",
    "    end = timeit.timeit()\n",
    "    print(f\"Linear tuning complete, time elapsed: {end-linear_start}\")\n",
    "    print(f\"Hyper-Parameter Optimisation Complete, time elapsed: {end-start}\\nRBF: {rbf_hyperparams}\\nPoly: {poly_hyperparams}\\nLinear: {linear_hyperparams}\")\n",
    "    \n",
    "    return rbf_hyperparams, rbf_val_history, poly_hyperparams, poly_val_history, linear_hyperparams, linear_val_history\n",
    "    \n",
    "perform_svm_analysis = False\n",
    "if perform_svm_analysis:\n",
    "    # Load data\n",
    "    try: # if already loaded, don't re-run\n",
    "        X_train\n",
    "        raw_X_train\n",
    "        Y_train\n",
    "        X_test\n",
    "        raw_X_test\n",
    "        Y_test\n",
    "    except Exception as ex:\n",
    "        print(f\"{ex} - generating embeddings...\")\n",
    "        raw_X_train, Y_train, raw_X_test, Y_test = load_data('movie_review_test.csv', 'movie_review_test.csv')\n",
    "        X_train = get_embeddings(raw_X_train)\n",
    "        X_test = get_embeddings(raw_X_test)\n",
    "    else:\n",
    "        print(\"embeddings already retrieved, skipping\")\n",
    "    \n",
    "    try: # if already loaded, don't re-run\n",
    "        rbf_hyperparams\n",
    "        rbf_val_history\n",
    "        poly_hyperparams\n",
    "        poly_val_history\n",
    "        linear_hyperparams\n",
    "        linear_val_history\n",
    "    except Exception as ex:\n",
    "        print(f\"{ex} - Optimising hyper-params...\")\n",
    "        rbf_hyperparams, rbf_val_history, poly_hyperparams, poly_val_history, linear_hyperparams, linear_val_history = svm_analysis(X_train, Y_train, X_test, Y_test)\n",
    "    else:\n",
    "        print(\"hyper-params already optimised, skipping\")\n",
    "        print(f\"Hyper-Parameters\\nRBF: {rbf_hyperparams}\\nPoly: {poly_hyperparams}\\nLinear: {linear_hyperparams}\")\n",
    "        \n",
    "    def save_to_csv(name, param_headers, data, data_extractor):\n",
    "        import csv\n",
    "        with open(name, 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file, delimiter=',')\n",
    "            writer.writerow(param_headers + ['KFold_Accuracy'])\n",
    "            writer.writerows([data_extractor(params) + [acc] for params, acc in data])\n",
    "    save_to_csv('rbf_tuning_final.csv', ['C', 'Gamma'], rbf_val_history, lambda params: [params[0], params[1]])\n",
    "    save_to_csv('poly_tuning_final.csv', ['C', 'Gamma', 'Degree'], poly_val_history, lambda params: [params[0], params[1], params[2]])\n",
    "    save_to_csv('lin_tuning_final.csv', ['C', 'x300', 'x55', 'x5', 'x317', 'x180', 'x177'], linear_val_history, lambda params: [params[0], params[1], params[2], params[3], params[4], params[5], params[6]])\n",
    "\n",
    "    # Plot graphs for hyper-param tuning\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from numpy import ma\n",
    "\n",
    "    def read_data(filename):\n",
    "        import csv\n",
    "        rows = []\n",
    "        with open(f'{filename}.csv') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            header = next(reader)\n",
    "            rows = [row in reader]\n",
    "        return rows\n",
    "\n",
    "    # Plot hyper-params search space for RBF\n",
    "    # best output - RBF: ((0.9000000000000001, 2.2857142857142856), 0.8206666666666667)\n",
    "    X = np.linspace(0.5, 3, 8) # Gamma\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, acc in rbf_val_history]).reshape((Y.shape[0], X.shape[0]))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(rbf_hyperparams[0][1], rbf_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.xlabel(\"Gamma\")\n",
    "    plt.ylabel(\"Kernel Coefficient\")\n",
    "    plt.title(\"Hyper-param tuning for RBF kernel\")\n",
    "    plt.show()\n",
    "\n",
    "    poly_rows = np.array([[c, g, d, acc] for (c, g, d), acc in poly_val_history])# np.array(read_data('poly_tuning_final'))\n",
    "    # best output - Poly: ((0.7, 2.2857142857142856, 2.0), 0.82)\n",
    "    poly_C = poly_hyperparams[0][0]\n",
    "    poly_gamma = poly_hyperparams[0][1]\n",
    "    poly_degree = poly_hyperparams[0][2]\n",
    "    poly_with_best_C = poly_rows[poly_rows[:, 0] == poly_C][:, [1, 2, 3]]\n",
    "    poly_with_best_gamma = poly_rows[poly_rows[:, 1] == poly_gamma][:, [0, 2, 3]]\n",
    "    poly_with_best_degree = poly_rows[poly_rows[:, 2] == poly_degree][:, [0, 1, 3]]\n",
    "\n",
    "    X = np.linspace(0.5, 3, 8) # Gamma\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, _, acc in poly_with_best_degree]).reshape((Y.shape[0], X.shape[0]))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(poly_hyperparams[0][1], poly_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.xlabel(\"Gamma\")\n",
    "    plt.ylabel(\"Kernel Coefficient\")\n",
    "    plt.title(\"Hyper-param tuning for Poly kernel (with Best Value For Degree)\")\n",
    "    plt.show()\n",
    "\n",
    "    X = np.linspace(1, 3, 3) # Degree\n",
    "    Y = np.linspace(0.5, 3, 8) # Gamma\n",
    "    Z = np.array([acc for _, _, acc in poly_with_best_C]).reshape((Y.shape[0], X.shape[0]))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(poly_hyperparams[0][1], poly_hyperparams[0][2], color='red', marker='x')\n",
    "    plt.ylabel(\"Gamma\")\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.title(\"Hyper-param tuning for Poly kernel (with Best Value For C)\")\n",
    "    plt.show()\n",
    "\n",
    "    X = np.linspace(1, 3, 3) # Degree\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, _, acc in poly_with_best_gamma]).reshape((Y.shape[0], X.shape[0]))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(poly_hyperparams[0][2], poly_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.ylabel(\"Kernel Coefficient\")\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.title(\"Hyper-param tuning for Poly kernel (with Best Value For Gamma)\")\n",
    "    plt.show()\n",
    "\n",
    "    # graphs for linear hyper-params\n",
    "    # best output - Linear: ((0.7, 3.0, 3.0, 1.0, 3.0, 5.0, 1.0), 0.8346666666666668)\n",
    "    linear_rows = np.array([[c, x1, x2, x3, x4, x5, x6, acc] for (c, x1, x2, x3, x4, x5, x6), acc in linear_val_history])\n",
    "    linear_x1 = linear_hyperparams[0][1]\n",
    "    linear_x2 = linear_hyperparams[0][2]\n",
    "    linear_x3 = linear_hyperparams[0][3]\n",
    "    linear_x4 = linear_hyperparams[0][4]\n",
    "    linear_x5 = linear_hyperparams[0][5]\n",
    "    linear_x6 = linear_hyperparams[0][6]\n",
    "    linear_rows_for_x1 = linear_rows[(linear_rows[:, 2] == linear_x2) & (linear_rows[:, 3] == linear_x3) & (linear_rows[:, 4] == linear_x4) & (linear_rows[:, 5] == linear_x5) & (linear_rows[:, 6] == linear_x6)][:, [0, 1, 7]]\n",
    "    linear_rows_for_x2 = linear_rows[(linear_rows[:, 1] == linear_x1) & (linear_rows[:, 3] == linear_x3) & (linear_rows[:, 4] == linear_x4) & (linear_rows[:, 5] == linear_x5) & (linear_rows[:, 6] == linear_x6)][:, [0, 2, 7]]\n",
    "    linear_rows_for_x3 = linear_rows[(linear_rows[:, 1] == linear_x1) & (linear_rows[:, 2] == linear_x2) & (linear_rows[:, 4] == linear_x4) & (linear_rows[:, 5] == linear_x5) & (linear_rows[:, 6] == linear_x6)][:, [0, 3, 7]]\n",
    "    linear_rows_for_x4 = linear_rows[(linear_rows[:, 1] == linear_x1) & (linear_rows[:, 2] == linear_x2) & (linear_rows[:, 3] == linear_x3) & (linear_rows[:, 5] == linear_x5) & (linear_rows[:, 6] == linear_x6)][:, [0, 4, 7]]\n",
    "    linear_rows_for_x5 = linear_rows[(linear_rows[:, 1] == linear_x1) & (linear_rows[:, 2] == linear_x2) & (linear_rows[:, 3] == linear_x3) & (linear_rows[:, 4] == linear_x4) & (linear_rows[:, 6] == linear_x6)][:, [0, 5, 7]]\n",
    "    linear_rows_for_x6 = linear_rows[(linear_rows[:, 1] == linear_x1) & (linear_rows[:, 2] == linear_x2) & (linear_rows[:, 3] == linear_x3) & (linear_rows[:, 4] == linear_x4) & (linear_rows[:, 5] == linear_x5)][:, [0, 6, 7]]\n",
    "\n",
    "    X = np.linspace(1, 5, 3) # Feature Weights\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, _, acc in linear_rows_for_x1]).reshape((Y.shape[0], len(X)))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.ylabel(\"X1\")\n",
    "    plt.xlabel(\"Kernel Coefficient\")\n",
    "    plt.title(\"Hyper-param tuning for Linear kernel (with Best Values for all parameters besides X1 and C)\")\n",
    "    plt.scatter(linear_hyperparams[0][1], linear_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.show()\n",
    "\n",
    "    X = np.linspace(1, 5, 3) # Feature Weights\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, _, acc in linear_rows_for_x2]).reshape((Y.shape[0], len(X)))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(linear_hyperparams[0][2], linear_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.xlabel(\"Kernel Coefficient\")\n",
    "    plt.title(\"Hyper-param tuning for Linear kernel (with Best Values for all parameters besides X2 and C)\")\n",
    "    plt.show()\n",
    "\n",
    "    X = np.linspace(1, 5, 3) # Feature Weights\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, _, acc in linear_rows_for_x3]).reshape((Y.shape[0], len(X)))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(linear_hyperparams[0][3], linear_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.ylabel(\"X3\")\n",
    "    plt.xlabel(\"Kernel Coefficient\")\n",
    "    plt.title(\"Hyper-param tuning for Linear kernel (with Best Values for all parameters besides X3 and C)\")\n",
    "    plt.show()\n",
    "\n",
    "    X = np.linspace(1, 5, 3) # Feature Weights\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, _, acc in linear_rows_for_x4]).reshape((Y.shape[0], len(X)))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(linear_hyperparams[0][4], linear_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.ylabel(\"X4\")\n",
    "    plt.xlabel(\"Kernel Coefficient\")\n",
    "    plt.title(\"Hyper-param tuning for Linear kernel (with Best Values for all parameters besides X4 and C)\")\n",
    "    plt.show()\n",
    "\n",
    "    X = np.linspace(1, 5, 3) # Feature Weights\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, _, acc in linear_rows_for_x5]).reshape((Y.shape[0], len(X)))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(linear_hyperparams[0][5], linear_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.ylabel(\"X5\")\n",
    "    plt.xlabel(\"Kernel Coefficient\")\n",
    "    plt.title(\"Hyper-param tuning for Linear kernel (with Best Values for all parameters besides X5 and C)\")\n",
    "    plt.show()\n",
    "\n",
    "    X = np.linspace(1, 5, 3) # Feature Weights\n",
    "    Y = np.linspace(2e-1, 1, 9) # C\n",
    "    Z = np.array([acc for _, _, acc in linear_rows_for_x6]).reshape((Y.shape[0], len(X)))\n",
    "    im = plt.contourf(X, Y, Z, 15)\n",
    "    plt.colorbar(im)\n",
    "    plt.scatter(linear_hyperparams[0][6], linear_hyperparams[0][0], color='red', marker='x')\n",
    "    plt.ylabel(\"X6\")\n",
    "    plt.xlabel(\"Kernel Coefficient\")\n",
    "    plt.title(\"Hyper-param tuning for Linear kernel (with Best Values for all parameters besides X6 and C)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # evaluate model performance for each kernel with best params\n",
    "    # TP | FP | TN | FN tables\n",
    "    # print-out sections it mis-identified\n",
    "    # Try to identify which features were responsible (e.g. typically strong in correct answers, but not in these?)\n",
    "    def generate_classification_table(prediction, actual):\n",
    "        def model_performance_metrics(tp, tn, fp, fn):\n",
    "            accuracy = (tp + tn) / (fp + tn + tp + fn)\n",
    "            f1_score = (2 * tp) / ((2 * tp) + fp + fn)\n",
    "            sensitivity = tp / (tp + fn)\n",
    "            false_negative_rate = fn / (tp + fn)\n",
    "            false_positive_rate = fp / (fp + tn)\n",
    "            specificity = tn / (fp + tn)\n",
    "            return {\n",
    "                'accuracy': accuracy,\n",
    "                'f1_score': f1_score,\n",
    "                'sensitivity': sensitivity,\n",
    "                'false_negative_rate': false_negative_rate,\n",
    "                'false_positive_rate': false_positive_rate,\n",
    "                'specificity': specificity\n",
    "            }\n",
    "        TP = []\n",
    "        TN = []\n",
    "        FP = []\n",
    "        FN = []\n",
    "        for idx, (pred, actual) in enumerate(zip(prediction, actual)):\n",
    "            if pred:\n",
    "                if actual:\n",
    "                    TP.append(idx)\n",
    "                else:\n",
    "                    FP.append(idx)\n",
    "            else:\n",
    "                if actual:\n",
    "                    FN.append(idx)\n",
    "                else:\n",
    "                    TN.append(idx)\n",
    "\n",
    "        model_performance = model_performance_metrics(len(TP), len(TN), len(FP), len(FN))\n",
    "        return (TP, TN, FP, FN, model_performance)\n",
    "\n",
    "    def save_misclassified_reviews(name, reviews, TP, TN, FP, FN):\n",
    "        with open(f\"{name}_TP.txt\", 'w') as tp_file:\n",
    "            for idx in TP:\n",
    "                tp_file.write(f\"{reviews[idx]}\\n\")\n",
    "        with open(f\"{name}_TN.txt\", 'w') as tp_file:\n",
    "            for idx in TN:\n",
    "                tp_file.write(f\"{reviews[idx]}\\n\")\n",
    "        with open(f\"{name}_FP.txt\", 'w') as tp_file:\n",
    "            for idx in FP:\n",
    "                tp_file.write(f\"{reviews[idx]}\\n\")\n",
    "        with open(f\"{name}_FN.txt\", 'w') as tp_file:\n",
    "            for idx in FN:\n",
    "                tp_file.write(f\"{reviews[idx]}\\n\")\n",
    "\n",
    "    rbf_model = SVMClassifier()\n",
    "    rbf_model.clf.set_params(**{'C': rbf_hyperparams[0][0], 'gamma': rbf_hyperparams[0][1]})\n",
    "    rbf_model.fit(X_train, Y_train)\n",
    "    rbf_Y_Pred = rbf_model.predict(X_test)\n",
    "    rbf_TP, rbf_TN, rbf_FP, rbf_FN, rbf_metrics = generate_classification_table(rbf_Y_Pred, Y_test)\n",
    "    print(f\"Correct Predicitons: TP: {len(rbf_TP)}, TN: {len(rbf_TN)}\\nIncorrect Predictions: FP: {len(rbf_FP)}, FN: {len(rbf_FN)}\\n metrics: {rbf_metrics}\")\n",
    "    save_misclassified_reviews('rbf', raw_X_train, rbf_TP, rbf_TN, rbf_FP, rbf_FN)\n",
    "\n",
    "    poly_model = SVMClassifier()\n",
    "    poly_model.clf.kernel = 'poly'\n",
    "    poly_model.clf.set_params(**{'C': poly_hyperparams[0][0], 'gamma': poly_hyperparams[0][1], 'degree': poly_hyperparams[0][2]})\n",
    "    poly_model.fit(X_train, Y_train)\n",
    "    poly_Y_Pred = poly_model.predict(X_test)\n",
    "    poly_TP, poly_TN, poly_FP, poly_FN, poly_metrics = generate_classification_table(poly_Y_Pred, Y_test)\n",
    "    print(f\"Correct Predicitons: TP: {len(poly_TP)}, TN: {len(poly_TN)}\\nIncorrect Predictions: FP: {len(poly_FP)}, FN: {len(poly_FN)}\\n metrics: {poly_metrics}\")\n",
    "    save_misclassified_reviews('poly', raw_X_train, poly_TP, poly_TN, poly_FP, poly_FN)\n",
    "\n",
    "    linear_model = SVMClassifier()\n",
    "    M = np.ones(X_train.shape[1])\n",
    "    params = [linear_hyperparams[0][1],linear_hyperparams[0][2],linear_hyperparams[0][3],linear_hyperparams[0][4],linear_hyperparams[0][5],linear_hyperparams[0][6]]\n",
    "    correlations_idx = [300, 55, 5, 317, 180, 177]\n",
    "    for m_idx, p_idx in zip(correlations_idx, range(1, len(params))):\n",
    "        M[m_idx] = params[p_idx]\n",
    "    linear_model.clf.kernel = linear_kernel_builder(M)\n",
    "    linear_model.clf.set_params(**{'C': linear_hyperparams[0][0]})\n",
    "    linear_model.fit(X_train, Y_train)\n",
    "    linear_Y_Pred = linear_model.predict(X_test)\n",
    "    linear_TP, linear_TN, linear_FP, linear_FN, linear_metrics = generate_classification_table(linear_Y_Pred, Y_test)\n",
    "    print(f\"Correct Predicitons: TP: {len(linear_TP)}, TN: {len(linear_TN)}\\nIncorrect Predictions: FP: {len(linear_FP)}, FN: {len(linear_FN)}\\n metrics: {linear_metrics}\")\n",
    "    save_misclassified_reviews('linear', raw_X_train, linear_TP, linear_TN, linear_FP, linear_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6f272",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89603f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit(X_train, Y_train)\n",
    "    Y_Pred = sc.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd4adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiffingj/bin/anaconda3/envs/ml2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "acc = test_func_svm(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61056292",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "In this task you need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "Details regarding the marking for the coursework are provided in the coursework specification file. Please ensure that your code will work with a different test file than the one provided with the coursework.\n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from scikit-learn or your own decision trees. Use the same sentiment analysis dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "import math\n",
    "import random\n",
    "\n",
    "class BoostingClassifier:\n",
    "    # You need to implement this classifier. \n",
    "    def __init__(self, num_classifiers = 50, max_depth = 3):\n",
    "        import numpy as np\n",
    "        #implement initialisation\n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.max_depth = max_depth\n",
    "        self.classifiers = []\n",
    "        self.alphas = np.ones(self.num_classifiers)\n",
    "        self.alphas /= self.num_classifiers\n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        signed_y = [1 if x == 1 else -1 for x in y]\n",
    "        num_samples = X.shape[0]\n",
    "        w = np.random.rand(num_samples)\n",
    "        w /= num_samples\n",
    "        alph = np.ones(self.num_classifiers)\n",
    "        for c in range(self.num_classifiers):\n",
    "            clf = tree.DecisionTreeClassifier(max_depth = self.max_depth)#splitter='random')\n",
    "            self.classifiers.append(clf)\n",
    "            clf.fit(X, signed_y, sample_weight = w)\n",
    "            \n",
    "            #get predictions and calculate error rate\n",
    "            predictions = clf.predict(X)     \n",
    "            error_rate = np.count_nonzero(predictions != signed_y) / num_samples\n",
    "            #set classifier weight\n",
    "            alpha = 0.5 * math.log((1-error_rate)/error_rate) if error_rate > 0 else 1\n",
    "            self.alphas[c]  = alpha\n",
    "            #update sample weights\n",
    "            \n",
    "            new_w = np.copy(w)\n",
    "            for i in range(num_samples):\n",
    "                w[i] = w[i] * math.exp(-1 * alpha * signed_y[i] * predictions[i])\n",
    "                \n",
    "            w /= np.sum(new_w)\n",
    "        \n",
    "        #implement training of the boosting classifier\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep= False):\n",
    "        return {'num_classifiers':self.num_classifiers, 'max_depth':self.max_depth}\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # implement prediction of the boosting classifier\n",
    "        h = []\n",
    "        accum = []\n",
    "        for x in X:\n",
    "            cumsum = 0\n",
    "            for c in range(self.num_classifiers):\n",
    "                cumsum += self.alphas[c] * self.classifiers[c].predict(np.array(x).reshape(1,-1))\n",
    "                accum.append(cumsum)\n",
    "            h.append(np.sign(cumsum))\n",
    "        return [0 if x < 0 else 1 for x in h]\n",
    "    \n",
    "perform_boosting_analysis = False\n",
    "if perform_boosting_analysis: \n",
    "    # Load data\n",
    "    try: # if already loaded, don't re-run\n",
    "        X_train\n",
    "        raw_X_train\n",
    "        Y_train\n",
    "        X_test\n",
    "        raw_X_test\n",
    "        Y_test\n",
    "    except Exception as ex:\n",
    "        print(f\"{ex} - generating embeddings...\")\n",
    "        raw_X_train, Y_train, raw_X_test, Y_test = load_data('movie_review_test.csv', 'movie_review_test.csv')\n",
    "        X_train = get_embeddings(raw_X_train)\n",
    "        X_test = get_embeddings(raw_X_test)\n",
    "    else:\n",
    "        print(\"embeddings already retrieved, skipping\")\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    def crossval_boost(model, x, y, kfolds = 5):\n",
    "        from sklearn.model_selection import KFold\n",
    "\n",
    "        kfold_accuracys = []\n",
    "\n",
    "        kf = KFold(n_splits = kfolds)\n",
    "        train_x = kf.split(x,y)\n",
    "\n",
    "\n",
    "        for train_index, val_index in kf.split(x):\n",
    "            train_x = x[train_index]\n",
    "            train_y = y[train_index]\n",
    "            val_x = x[val_index]\n",
    "            val_y = y[val_index]\n",
    "\n",
    "            bc = BoostingClassifier()\n",
    "            bc.fit(train_x, train_y)\n",
    "            Y_Pred = bc.predict(val_x)\n",
    "            acc = accuracy_score(val_y, Y_Pred)\n",
    "\n",
    "            kfold_accuracys.append(acc)\n",
    "\n",
    "        return np.mean(kfold_accuracys)\n",
    "    \n",
    "    ## Hyperparam tuning for gradboost\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint\n",
    "    from scipy.stats import uniform\n",
    "    def grid_search(params):\n",
    "        X_train, y_train, X_test, Y_test = extract_bag_of_words_train_test(\"movie_review_train.csv\", \"movie_review_test.csv\")\n",
    "        clfs = []\n",
    "        best_acc = 0\n",
    "\n",
    "        for num in params['num_classifiers']:\n",
    "            for max_depth in params['max_depth']:\n",
    "                clfs.append(BoostingClassifier(num, max_depth))\n",
    "        for clf in clfs:\n",
    "            accuracy = crossval_boost(clf, X_train, np.array(y_train), 5)\n",
    "            print('Acc:' , accuracy, 'Params:', clf.num_classifiers, clf.max_depth)\n",
    "            if accuracy > best_acc:\n",
    "                best_clf = clf\n",
    "                best_acc = accuracy\n",
    "\n",
    "        print(\"Best Params:\", best_clf.num_classifiers, best_clf.max_depth)\n",
    "        return best_clf\n",
    "\n",
    "    \n",
    "    \n",
    "    try: # if already loaded, don't re-run\n",
    "        best_clf\n",
    "    except Exception as ex:\n",
    "        print(f\"{ex} - running Hyper-parameter optimisation\")\n",
    "        grid = {'num_classifiers': [50, 100, 200, 500], 'max_depth': [2, 5, 10, 25]}\n",
    "        best_clf = grid_search(grid)\n",
    "    else:\n",
    "        print(\"already optimised, skipping...\")\n",
    "    \n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "\n",
    "    Y_Pred = bc.predict(X_test)\n",
    "    print(raw_X_test)\n",
    "\n",
    "    correct_indices = np.nonzero(np.equal(Y_Pred, Y_test))\n",
    "    misclassified_indices = np.nonzero(np.not_equal(Y_Pred, Y_test))\n",
    "\n",
    "    correct_samples = raw_X_test.iloc[correct_indices]\n",
    "    misclassified_samples = raw_X_test.iloc[misclassified_indices]\n",
    "\n",
    "    print(correct_samples)\n",
    "    print(misclassified_samples)\n",
    "\n",
    "    correct_reviews = correct_samples['review']\n",
    "    misclassified_reviews = misclassified_samples['review']\n",
    "\n",
    "    correct_lengths = list(map(lambda x: len(x.split(' ')), correct_reviews))\n",
    "    misclass_lengths = list(map(lambda x: len(x.split(' ')), misclassified_reviews))\n",
    "\n",
    "    correct_sentiment = [1 if x == 'positive' else 0 for x in correct_samples['sentiment']]\n",
    "    misclass_sentiment = [1 if x == 'positive' else 0 for x in misclassified_samples['sentiment']]\n",
    "\n",
    "    plt.hist(correct_lengths)\n",
    "    plt.hist(misclass_lengths)\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(correct_samples['sentiment'])\n",
    "    plt.show()\n",
    "    plt.hist(misclassified_samples['sentiment'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e0987",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting(dataset_train, dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train, X_test, Y_test) = extract_bag_of_words_train_test(dataset_train, dataset_test)\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    Y_Pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, Y_Pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c27de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = test_func_boosting(\"movie_review_train.csv\", \"movie_review_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
