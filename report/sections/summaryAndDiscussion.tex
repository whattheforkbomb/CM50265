\section{Summary and Conclusions}
Though Model A produced better values for the age Mean Square Error during training, it was out-performed by Model B during validation (for both age and gender loss metrics), and during training when viewing the gender Binary Accuracy.

Both models were observed to suffer from over-fitting to some degree.
Model A suffered over-fitting for both branches, while Model B showed over-fitting just for the gender branch.\\
This is evidenced by the diverging of the learning curves between the training and validation metrics, despite measures to counteract over-fitting, including: L2 regularisation, dropout, and data augmentation.

Given the top-1 and top-5 accuracy observed from ResNet50 on the ImageNet dataset, you'd presume that it should handedly outperform a much smaller/simpler model, since it has been trained to extract the best features for a 1000s of classes.\\
Below are the reasons we believe the performance was so close.

\textbfit{Simple vs Complex}\\
Despite having a much simpler architecture and many fewer parameters, Model A was built to solely predict age and gender. Model B by comparison was trained on ImageNet to classify images as from 1000's of classes. 
None of these classes are related to human faces so the power of the ResNet50 base model is applicable only to more generic image features in our case.\\
Although we did unfreeze the ResNet50 model weights, with the intention of allowing the ResNet50 convolution layer weights to be optimised for the task at hand, it may simply be the the model itself doesn't lend itself to extracting the features we need.\\
We can see that Model B seemed to 'learn' faster, shown by the initial age and gender metrics being better, and then converging much sooner than Model A.\\
This could indicate that ResNet50 convolutions layers allowed better feature extraction, but there was some other limitation preventing either model from getting better performance without over-fitting.

\textbfit{Lack Of Data Diversity / Small Dataset Size}\\
With the provided dataset size and diversity, it is possible that the models are unable to learn enough general features from the data to aptly predict the age and gender.

This could also explain the over-fitting issues observed on both models, since the models could be learning the training data too specifically due to a lack of diversity.\\
We could simplify the models, however there were several hyper-parameters tied to model complexity (convolution layer feature depth, FCN layer unit count) which were tuned to their specific values based on them producing better validation loss.\\
